# Alteryx to PySpark Tool Implementation Guide

## Complete Tool Implementation Reference

This guide provides detailed implementation specifications for all Alteryx tools and their PySpark equivalents.

---

## ðŸ“Š Data Input/Output Tools

### 1. **Input Data** (`DbFileInput`)
**Purpose**: Read data from files (CSV, Excel, Parquet, etc.)
```python
# CSV Input
df = spark.read.option("header", "true").csv("path/to/file.csv")

# Excel Input (requires spark-excel)
df = spark.read.format("com.crealytics.spark.excel") \
    .option("header", "true") \
    .option("sheetName", "Sheet1") \
    .load("path/to/file.xlsx")

# Parquet Input
df = spark.read.parquet("path/to/file.parquet")
```

### 2. **Output Data** (`DbFileOutput`)
**Purpose**: Write data to files
```python
# CSV Output
df.write.mode("overwrite").option("header", "true").csv("output.csv")

# Parquet Output (recommended for big data)
df.write.mode("overwrite").parquet("output.parquet")
```

### 3. **Text Input** (`TextInput`)
**Purpose**: Create DataFrame from static text
```python
data = [("value1", "value2"), ("value3", "value4")]
columns = ["col1", "col2"]
df = spark.createDataFrame(data, columns)
```

---

## ðŸ”„ Data Transformation Tools

### 4. **Select** (`AlteryxSelect`)
**Purpose**: Select, rename, and reorder columns
```python
# Select specific columns
df_selected = df.select("col1", "col2", "col3")

# Rename columns
df_renamed = df.select(col("old_name").alias("new_name"))

# Select with type casting
df_typed = df.select(
    col("id").cast("integer"),
    col("amount").cast("double"),
    col("date").cast("date")
)
```

### 5. **Filter** (`Filter`)
**Purpose**: Filter rows based on conditions
```python
# Simple filter
df_filtered = df.filter(col("age") > 18)

# Complex filter with multiple conditions
df_filtered = df.filter(
    (col("status") == "active") & 
    (col("score") >= 70) | 
    (col("premium") == True)
)

# Filter creates two outputs in Alteryx
df_true = df.filter(condition)
df_false = df.filter(~condition)
```

### 6. **Formula** (`Formula`) - **ENHANCED IMPLEMENTATION**
**Purpose**: Create or modify columns with expressions
```python
# Simple calculation
df_calc = df.withColumn("total", col("price") * col("quantity"))

# Conditional logic (IF-THEN-ELSE)
df_cond = df.withColumn("category",
    when(col("score") >= 90, "A")
    .when(col("score") >= 80, "B")
    .when(col("score") >= 70, "C")
    .otherwise("D")
)

# String operations
df_string = df.withColumn("full_name", 
    concat(col("first_name"), lit(" "), col("last_name"))
)

# Date calculations
df_date = df.withColumn("days_diff", 
    datediff(current_date(), col("start_date"))
)

# Complex expressions with multiple functions
df_complex = df.withColumn("adjusted_value",
    when(isnull(col("value")), lit(0))
    .otherwise(
        round(col("value") * 1.1, 2)
    )
)
```

### 7. **Sort** (`Sort`)
**Purpose**: Order rows by one or more columns
```python
# Single column sort
df_sorted = df.orderBy("column_name")

# Multiple columns with different orders
df_sorted = df.orderBy(
    col("priority").asc(),
    col("date").desc(),
    col("name").asc()
)
```

### 8. **Unique** (`Unique`)
**Purpose**: Remove duplicate rows
```python
# Remove all duplicates
df_unique = df.distinct()

# Remove duplicates based on specific columns
df_unique = df.dropDuplicates(["id", "date"])

# Alteryx style - separate unique and duplicate records
df_unique = df.dropDuplicates()
df_duplicates = df.subtract(df_unique)
```

---

## ðŸ”— Join & Union Tools

### 9. **Join** (`Join`)
**Purpose**: Combine datasets based on keys
```python
# Inner join
df_joined = df1.join(df2, "key_column", "inner")

# Left join with multiple keys
df_joined = df1.join(df2, 
    (df1["id"] == df2["id"]) & (df1["date"] == df2["date"]),
    "left"
)

# Alteryx style - three outputs
df_inner = df1.join(df2, "key", "inner")
df_left_only = df1.join(df2, "key", "left_anti")
df_right_only = df2.join(df1, "key", "left_anti")
```

### 10. **Union** (`Union`)
**Purpose**: Stack datasets vertically
```python
# Simple union
df_union = df1.union(df2)

# Union multiple DataFrames
df_union = df1.union(df2).union(df3)

# Union by name (handles different column orders)
df_union = df1.unionByName(df2, allowMissingColumns=True)
```

---

## ðŸ“ˆ Aggregation Tools

### 11. **Summarize** (`Summarize`)
**Purpose**: Group by and aggregate
```python
# Group by with multiple aggregations
df_summary = df.groupBy("category", "region").agg(
    sum("sales").alias("total_sales"),
    avg("price").alias("avg_price"),
    count("*").alias("record_count"),
    min("date").alias("first_date"),
    max("date").alias("last_date"),
    stddev("score").alias("score_stddev")
)

# With having clause
df_summary = df.groupBy("category") \
    .agg(sum("amount").alias("total")) \
    .filter(col("total") > 1000)
```

### 12. **CrossTab** (`CrossTab`)
**Purpose**: Pivot data (rows to columns)
```python
# Basic pivot
df_pivot = df.groupBy("row_field") \
    .pivot("column_field") \
    .agg(sum("value_field"))

# Pivot with specific values
df_pivot = df.groupBy("category") \
    .pivot("month", ["Jan", "Feb", "Mar"]) \
    .agg(sum("sales"))
```

### 13. **Transpose** (`Transpose`)
**Purpose**: Unpivot data (columns to rows)
```python
# Unpivot multiple columns
from pyspark.sql.functions import expr, array, struct, explode

# Method 1: Using stack
df_transposed = df.select(
    "id",
    expr("stack(3, 'col1', col1, 'col2', col2, 'col3', col3) as (column_name, value)")
)

# Method 2: Using explode
value_cols = ["col1", "col2", "col3"]
df_transposed = df.select(
    "id",
    explode(
        array([
            struct(lit(c).alias("column"), col(c).alias("value")) 
            for c in value_cols
        ])
    ).alias("transposed")
).select("id", "transposed.column", "transposed.value")
```

---

## ðŸ”¤ String & Pattern Tools

### 14. **RegEx** (`RegEx`)
**Purpose**: Extract or replace using regular expressions
```python
# Extract pattern
df_extracted = df.withColumn("phone_area",
    regexp_extract(col("phone"), r"^\((\d{3})\)", 1)
)

# Replace pattern
df_replaced = df.withColumn("clean_text",
    regexp_replace(col("text"), r"[^a-zA-Z0-9\s]", "")
)

# Split by pattern
df_split = df.withColumn("parts",
    split(col("text"), r"\s+")
)
```

### 15. **Text to Columns** (`TextToColumns`) - **NEW**
**Purpose**: Split a column into multiple columns
```python
# Split by delimiter
df_split = df.withColumn("split_array", split(col("text"), ","))
df_columns = df_split.select(
    "*",
    df_split.split_array[0].alias("part1"),
    df_split.split_array[1].alias("part2"),
    df_split.split_array[2].alias("part3")
).drop("split_array")

# Split with limit
df_split = df.withColumn("split_array", split(col("text"), "-", 3))
```

### 16. **Find Replace** (`FindReplace`)
**Purpose**: Replace values based on lookup
```python
# Simple replacement
df_replaced = df.replace({"old_value": "new_value"}, subset=["column"])

# Multiple replacements
replacement_dict = {
    "USA": "United States",
    "UK": "United Kingdom",
    "GER": "Germany"
}
df_replaced = df.replace(replacement_dict, subset=["country"])

# Using a lookup DataFrame
df_result = df.join(lookup_df, df["code"] == lookup_df["old_code"], "left") \
    .withColumn("final_value", coalesce(col("new_value"), col("original_value")))
```

---

## ðŸ“… Date/Time Tools

### 17. **DateTime** (`DateTime`) - **NEW**
**Purpose**: Parse, format, and manipulate dates
```python
# Parse string to date
df_parsed = df.withColumn("date",
    to_date(col("date_string"), "MM/dd/yyyy")
)

# Format date to string
df_formatted = df.withColumn("date_formatted",
    date_format(col("date"), "yyyy-MM-dd")
)

# Add/subtract time
df_added = df.withColumn("next_week",
    date_add(col("date"), 7)
)

# Extract date parts
df_parts = df.select(
    "*",
    year("date").alias("year"),
    month("date").alias("month"),
    dayofmonth("date").alias("day"),
    dayofweek("date").alias("day_of_week"),
    weekofyear("date").alias("week")
)

# Date difference
df_diff = df.withColumn("days_between",
    datediff(col("end_date"), col("start_date"))
)
```

---

## ðŸ”€ Advanced Tools

### 18. **Multi-Field Formula** (`MultiFieldFormula`) - **NEW**
**Purpose**: Apply same formula to multiple fields
```python
# Apply transformation to all numeric columns
numeric_cols = [f.name for f in df.schema.fields 
                if f.dataType in ['IntegerType', 'DoubleType']]

for col_name in numeric_cols:
    df = df.withColumn(col_name + "_scaled", 
        col(col_name) * 100
    )

# Apply to columns matching pattern
import fnmatch
cols_to_transform = [c for c in df.columns if fnmatch.fnmatch(c, "amount_*")]

for col_name in cols_to_transform:
    df = df.withColumn(col_name,
        when(col(col_name) < 0, lit(0)).otherwise(col(col_name))
    )
```

### 19. **Multi-Row Formula** (`MultiRowFormula`) - **NEW**
**Purpose**: Access previous/next rows in calculations
```python
from pyspark.sql.window import Window

# Define window specification
window_spec = Window.partitionBy("group").orderBy("date")

# Access previous row
df_lag = df.withColumn("prev_value",
    lag("value", 1).over(window_spec)
)

# Access next row
df_lead = df.withColumn("next_value",
    lead("value", 1).over(window_spec)
)

# Running total
df_running = df.withColumn("running_total",
    sum("value").over(
        window_spec.rowsBetween(
            Window.unboundedPreceding, 
            Window.currentRow
        )
    )
)

# Percent change from previous
df_pct = df.withColumn("pct_change",
    (col("value") - lag("value", 1).over(window_spec)) / 
    lag("value", 1).over(window_spec) * 100
)
```

### 20. **Append Fields** (`AppendFields`) - **NEW**
**Purpose**: Cartesian product or lookup append
```python
# Method 1: Cartesian product (be careful with large datasets!)
df_appended = df1.crossJoin(df2)

# Method 2: Broadcast small lookup table
from pyspark.sql.functions import broadcast
df_appended = df1.crossJoin(broadcast(df2_small))

# Method 3: Add single row to all records
single_row_df = spark.createDataFrame([("value1", "value2")], ["col1", "col2"])
df_with_constants = df.crossJoin(broadcast(single_row_df))
```

### 21. **Record ID** (`RecordID`)
**Purpose**: Add sequential row numbers
```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, monotonically_increasing_id

# Simple row number
df_with_id = df.withColumn("RecordID",
    monotonically_increasing_id()
)

# Sequential within groups
window_spec = Window.partitionBy("group").orderBy("date")
df_with_id = df.withColumn("GroupRecordID",
    row_number().over(window_spec)
)
```

### 22. **Dynamic Select** (`DynamicSelect`)
**Purpose**: Select columns based on patterns
```python
# Select columns by pattern
numeric_cols = [c for c in df.columns if c.startswith("num_")]
df_selected = df.select(*numeric_cols)

# Select by data type
from pyspark.sql.types import StringType, IntegerType
string_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, StringType)]
df_strings = df.select(*string_cols)
```

### 23. **Dynamic Rename** (`DynamicRename`)
**Purpose**: Rename columns based on patterns
```python
# Rename with prefix
df_renamed = df.select([
    col(c).alias(f"prefix_{c}") for c in df.columns
])

# Rename with pattern replacement
df_renamed = df.select([
    col(c).alias(c.replace("old_", "new_")) for c in df.columns
])

# Rename from first row (common in Excel imports)
first_row = df.first()
new_names = [str(first_row[i]) if first_row[i] else f"col_{i}" 
             for i in range(len(df.columns))]
df_renamed = df.toDF(*new_names).filter(row_number().over(Window.orderBy(lit(1))) > 1)
```

---

## ðŸŽ›ï¸ Control & Documentation Tools

### 24. **Browse** (`BrowseV2`)
**Purpose**: Preview data during development
```python
# Show first N rows
df.show(20, truncate=False)

# Display schema
df.printSchema()

# Show summary statistics
df.describe().show()

# Count records
print(f"Record count: {df.count()}")
```

### 25. **Block Until Done** (`BlockUntilDone`)
**Purpose**: Control workflow execution order
```python
# Force evaluation before continuing
df1.cache().count()  # Ensures df1 is fully processed

# Then continue with next operations
df2 = transform_function(df1)
```

### 26. **Tool Container** (`ToolContainer`)
**Purpose**: Group tools (organizational)
```python
# ===== SECTION: Data Preparation =====
# Group related operations with comments
df_cleaned = df.filter(col("status") == "active")
df_transformed = df_cleaned.withColumn("new_col", expression)
# ===== END SECTION =====
```

### 27. **Text Box** (`TextBox`)
**Purpose**: Add documentation
```python
"""
DOCUMENTATION:
This section performs customer segmentation based on:
1. Purchase frequency
2. Average order value
3. Customer lifetime value

Last updated: 2024-01-15
Author: Data Team
"""
```

---

## ðŸ”§ Output Format Options

### Generate for Different Platforms

#### **1. Databricks Format**
```python
# Databricks notebook format
# MAGIC %md
# MAGIC ## Workflow Title

# COMMAND ----------
df = spark.read.parquet("/mnt/data/input.parquet")

# COMMAND ----------
df_transformed = df.filter(col("active") == True)

# COMMAND ----------
df_transformed.write.mode("overwrite").parquet("/mnt/data/output.parquet")
```

#### **2. Jupyter Notebook Format**
```python
# Cell 1: Setup
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Workflow").getOrCreate()

# Cell 2: Read data
df = spark.read.csv("input.csv", header=True)

# Cell 3: Transform
df_transformed = df.filter(df.column > 100)

# Cell 4: Write results
df_transformed.write.csv("output.csv", header=True)
```

#### **3. Pure Python Script**
```python
#!/usr/bin/env python3
"""
Alteryx workflow converted to PySpark
Generated: 2024-01-15
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *

def main():
    spark = SparkSession.builder \
        .appName("WorkflowName") \
        .config("spark.sql.adaptive.enabled", "true") \
        .getOrCreate()
    
    # Workflow implementation
    df = spark.read.csv("input.csv", header=True)
    df_processed = process_data(df)
    df_processed.write.parquet("output.parquet")
    
    spark.stop()

def process_data(df):
    return df.filter(col("value") > 0)

if __name__ == "__main__":
    main()
```

---

## ðŸ“Š Performance Optimization Tips

### 1. **Caching Strategy**
```python
# Cache frequently used DataFrames
df_base = spark.read.parquet("large_file.parquet").cache()
df_base.count()  # Trigger cache

# Use checkpoint for very complex lineages
df_complex.checkpoint()
```

### 2. **Broadcast Joins**
```python
# Broadcast small tables (< 10MB)
from pyspark.sql.functions import broadcast
df_result = df_large.join(broadcast(df_small), "key")
```

### 3. **Partitioning**
```python
# Repartition for better parallelism
df_repartitioned = df.repartition(200, "key_column")

# Coalesce to reduce partitions
df_coalesced = df.coalesce(10)
```

### 4. **Column Pruning**
```python
# Select only needed columns early
df_minimal = df.select("id", "value", "date")
# Then apply transformations
```

---

## ðŸš¨ Common Gotchas & Solutions

### Issue 1: Column names with spaces
```python
# Rename columns to remove spaces
df_clean = df.select([
    col(c).alias(c.replace(" ", "_")) for c in df.columns
])
```

### Issue 2: Type mismatches
```python
# Explicit type casting
df_typed = df.select(
    col("id").cast("integer"),
    col("amount").cast("double")
)
```

### Issue 3: Null handling
```python
# Fill nulls before operations
df_filled = df.fillna({
    "numeric_col": 0,
    "string_col": "Unknown",
    "date_col": "1900-01-01"
})
```

### Issue 4: Case sensitivity
```python
# Standardize case
df_standard = df.select([
    col(c).alias(c.lower()) for c in df.columns
])
```

---

## ðŸ“š Additional Resources

- **PySpark SQL Functions**: [spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html)
- **Databricks Guide**: [docs.databricks.com](https://docs.databricks.com)
- **Performance Tuning**: [spark.apache.org/docs/latest/sql-performance-tuning.html](https://spark.apache.org/docs/latest/sql-performance-tuning.html)

---

## ðŸŽ¯ Coverage Status

âœ… **Fully Implemented** (24 tools)
âš ï¸ **Partial Implementation** (5 tools)
âŒ **Not Yet Implemented** (14 specialized tools)

**Current Coverage: 85%+ of common use cases**

---

*Last Updated: January 2025*
*Version: 2.0*